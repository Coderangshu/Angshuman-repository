{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\n",
    "\n",
    "We may also want to develop some early understanding about the domain of our problem. This is described on the Kaggle competition description page.\n",
    "\n",
    "On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\n",
    "Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Question or problem definition.\n",
    "* Acquire training and testing data.\n",
    "* Wrangle, prepare, cleanse the data.\n",
    "* Analyze, identify patterns, and explore the data.\n",
    "* Model, predict and solve the problem.\n",
    "* Visualize, report, and present the problem solving steps and final solution.\n",
    "* Supply or submit the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d8f90478b4d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Data Visualization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\seaborn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpalettes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mrelational\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mregression\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\seaborn\\relational.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0malgorithms\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbootstrap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpalettes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcolor_palette\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcubehelix_palette\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_parse_cubehelix_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0maxisgrid\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFacetGrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_facet_docs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\seaborn\\axisgrid.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpalettes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcolor_palette\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblend_palette\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mexternal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msix\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkdeplot\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0m_freedman_diaconis_bins\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\seaborn\\distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonparametric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msmnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0m_has_statsmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\statsmodels\\nonparametric\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbandwidths\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mkernel_density\u001b[0m \u001b[1;32mimport\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mKDEMultivariate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKDEMultivariateConditional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEstimatorSettings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mkernel_regression\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKernelReg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKernelCensoredReg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\statsmodels\\nonparametric\\kernel_density.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkernels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_kernel_base\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGenericKDE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEstimatorSettings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpke\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mLeaveOneOut\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_adjust_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\statsmodels\\nonparametric\\_kernel_base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mhas_joblib\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMemorizedResult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregister_store_backend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPrintTime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# Local imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhashing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfunc_inspect\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_func_code\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_func_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfunc_inspect\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mformat_call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfunc_inspect\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mformat_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\func_inspect.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_compat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_basestring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_memory_helpers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mopen_py_source\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_compat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPY3_OR_LATER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_isfile\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_is_mode_type\u001b[1;34m(path, mode)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../input/titanic/test.csv\")\n",
    "train_df = pd.read_csv(\"../input/titanic/train.csv\")\n",
    "combine = [train_df,test_df]\n",
    "train_df.info()\n",
    "print('_'*40)\n",
    "test_df.info()\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Categorical:** Survived, Sex, and Embarked. Ordinal: Pclass.\n",
    "* **Continous:** Age, Fare. Discrete: SibSp, Parch.\n",
    "* Seven features are integer or floats. Six in case of test dataset.\n",
    "* Five features are strings (object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Missing values\n",
    "print('Train columns with null values:\\n', train_df.isnull().sum())\n",
    "print(\"-\"*40)\n",
    "print('Test columns with null values:\\n', test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Train columns with null values:** Cabin > Age > Embarked \n",
    "* **Test columns with null values:** Cabin > Age > Fare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The distribution of numerical feature values across the samples:**\n",
    "* Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\n",
    "* The sample sruvival rate is around 38%.\n",
    "* Fares varied significantly with few passengers (<1%) paying as high as $512.\n",
    "* Few elderly passengers (<1%) within age range 65-80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe(include='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The distribution of categorical features:**\n",
    "* Names are unique across the dataset (count=unique=891)\n",
    "* Sex variable as two possible values with 65% male (top=male, freq=577/count=891).\n",
    "* Ticket feature has high ratio (22%) of duplicate values (unique=681).\n",
    "* Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.(147/204)\n",
    "* Embarked takes three possible values. S port used by most passengers (top=S)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we consider and explore several assumption factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(train_df, col='Survived')\n",
    "g.map(plt.hist, 'Age', bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Most passengers are in 15-35 age range\n",
    "* Large number of passengers in age range(15-30) didn't survive.\n",
    "* Infants (Age <=4) had high survival rate.\n",
    "* Oldest passengers (Age = 80) survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train_df,\n",
    "                height=6, kind=\"bar\", palette=\"muted\")\n",
    "g.despine(left=True)\n",
    "g.set_ylabels(\"survival probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In all classes, most survived passenegers are female.\n",
    "* The survival rate of female is much higher than males'.\n",
    "* The survival rate decreased from class 1 to class 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\n",
    "grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n",
    "grid.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pclass=3 had most passengers, however most did not survive..\n",
    "* Most passengers in Pclass=1 survived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing and Exploration\n",
    "\n",
    "missing data, new features, converting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing data; Drop columns.\n",
    "for dataset in combine:    \n",
    "    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n",
    "    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n",
    "    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n",
    "    \n",
    "drop_column = ['PassengerId','Cabin', 'Ticket']\n",
    "train_df.drop(drop_column, axis=1, inplace = True)\n",
    "test_id=test_df['PassengerId']\n",
    "test_df.drop(drop_column, axis=1, inplace = True)\n",
    "print('Train columns with null values:\\n', train_df.isnull().sum())\n",
    "print(\"-\"*40)\n",
    "print('Test columns with null values:\\n', test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyze by pivoting features**\n",
    "\n",
    "To confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_df.columns[1:9]:\n",
    "    if train_df[x].dtype != 'float64' :\n",
    "        print('Survival Correlation by:', x)\n",
    "        print(train_df[[x,\"Survived\"]].groupby(x, as_index=False).mean().sort_values(by='Survived', ascending=False))\n",
    "        print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Pclass : ** We observe significant correlation (>0.5) among Pclass=1 and Survived.We decide to include this feature in our model.\n",
    "* **Name : ** Name values were mixed texture, we can extract new feature \"Title\" based on this.\n",
    "* **Sex : ** Sex=female had very high survival rate at 74%.\n",
    "* **SibSp and Parch : ** These features had zero correlation for certain values. We can derive features from these individual features.\n",
    "* **Embarked : ** Embarked=C had higher survival rate at 55%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name --> Title\n",
    "#extract these.count less than 10 with title = \"Rare\"\n",
    "for dataset in combine:  \n",
    "    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "    title_names = (dataset['Title'].value_counts() < 10)\n",
    "    dataset['Title'] = dataset['Title'].apply(lambda x: 'Rare' if title_names.loc[x] == True else x)\n",
    "    dataset.drop(['Name'], axis=1, inplace = True)\n",
    "\n",
    "print('Train Count of Titles:\\n',train_df['Title'].value_counts())\n",
    "print('-'*40)\n",
    "print('Test Count of Titles:\\n',test_df['Title'].value_counts())\n",
    "print('-'*40)\n",
    "print('Train title with null values:\\n', train_df[\"Title\"].isnull().sum())\n",
    "print(\"-\"*40)\n",
    "print('Test title with null values:\\n', test_df[\"Title\"].isnull().sum())\n",
    "print(\"-\"*40)\n",
    "print(train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new feature (Family Size/ IsAlone) combining existing features (SibSp/ Parch) \n",
    "for dataset in combine:\n",
    "    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1   #Discrete variables\n",
    "    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n",
    "    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n",
    "    drop_column = ['SibSp','Parch','FamilySize']\n",
    "    dataset.drop(drop_column, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Fare and Age bands (reduce the effects of minor observation errors.)\n",
    "for dataset in combine:\n",
    "    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n",
    "    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n",
    "print(train_df[['FareBin', 'Survived']].groupby(['FareBin'], as_index=False).mean().sort_values(by='FareBin', ascending=True))\n",
    "print(\"-\"*40)\n",
    "print(train_df[['AgeBin', 'Survived']].groupby(['AgeBin'], as_index=False).mean().sort_values(by='AgeBin', ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Fare and Age with ordinals based on these bands.\n",
    "for dataset in combine:    \n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    " #--------------------------------------------------------------------------------------   \n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    " #--------------------------------------------------------------------------------------      \n",
    "    drop_column = ['FareBin','AgeBin']\n",
    "    dataset.drop(drop_column, axis=1, inplace = True)\n",
    "\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the categorical values (Title/ Sex/ Embarked) to ordinal.\n",
    "#That categorical data is defined as variables with a finite set of label values. \n",
    "#That most machine learning algorithms require numerical input and output variables. \n",
    "#That an integer and one hot encoding is used to convert categorical data to integer data.\n",
    "for dataset in combine:\n",
    "    dataset['Title'] = dataset['Title'].map({\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}).astype(int)\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "    \n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation\n",
    "for x in train_df.columns[1:8]:\n",
    "    if train_df[x].dtype != 'float64' :\n",
    "        print('Survival Correlation by:', x)\n",
    "        print(train_df[[x,\"Survived\"]].groupby(x, as_index=False).mean().sort_values(by='Survived', ascending=False))\n",
    "        print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive coefficients increase the log-odds of the response (and thus increase the probability).\n",
    "#Negative coefficients decrease the log-odds of the response (and thus decrease the probability).\n",
    "correlation = train_df.corr()\n",
    "plt.figure(figsize=(10,8))\n",
    "mask = np.zeros_like(correlation)#https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(correlation,linewidths=.3,annot=True,mask=mask,cmap=\"YlGnBu\",cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sex had highest correlation with Survived.\n",
    "* Title was second highest positive correlation. and it's related with Sex and Fare.\n",
    "* Pclass had negative correlation with survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph individual features by survival\n",
    "fig, saxis = plt.subplots(2, 3,figsize=(14,10))\n",
    "list1=['Pclass', 'Sex', 'Age', 'Fare', 'Embarked','IsAlone'];list2=[0,0,0,1,1,1];list3=[0,1,2,0,1,2]\n",
    "for (x,y,z) in zip(list1,list2,list3): \n",
    "    sns.barplot(x = x, y = 'Survived', data=train_df, ax = saxis[y,z])\n",
    "    print(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above graph show that in each conditions, which type of passenger had higher survival rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model, predict and solve\n",
    "\n",
    "The purpose of machine learning is to solve human problems.Machine learning can be categorized as: supervised learning, unsupervised learning, and reinforced learning. \n",
    "\n",
    "    Supervised learning is where you train the model by presenting it a training dataset that includes the correct answer. \n",
    "\n",
    "    Unsupervised learning is where you train the model using a training dataset that does not include the correct answer.\n",
    "   \n",
    "\n",
    "We are doing supervised machine learning, because we are training our algorithm by presenting it with a set of features and their corresponding target.There are many machine learning algorithms, however they can be reduced to four categories: classification, regression, clustering, or dimensionality reduction, depending on your target variable and data modeling goals.\n",
    "\n",
    "We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We can generalize that a continuous target variable requires a regression algorithm and a discrete target variable requires a classification algorithm. So our problem is a classification and regression problem. We can narrow down our choice of models to a few. These include:\n",
    "\n",
    "* Logistic Regression\n",
    "* KNN or k-Nearest Neighbors\n",
    "* Support Vector Machines\n",
    "* Naive Bayes classifier\n",
    "* Decision Tree\n",
    "* Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(\"Survived\", axis=1)\n",
    "Y_train = train_df[\"Survived\"]\n",
    "X_test  = test_df.copy()\n",
    "X_train.shape, Y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "Y_pred1 = logreg.fit(X_train, Y_train).predict(X_test)\n",
    "acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n",
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors = 4)\n",
    "Y_pred2 = knn.fit(X_train, Y_train).predict(X_test)\n",
    "acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n",
    "# Support Vector Machines\n",
    "svm = SVC()\n",
    "Y_pred3 = svm.fit(X_train, Y_train).predict(X_test)\n",
    "acc_svm = round(svm.score(X_train, Y_train) * 100, 2)\n",
    "# Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "Y_pred4 = nb.fit(X_train, Y_train).predict(X_test)\n",
    "acc_nb = round(nb.score(X_train, Y_train) * 100, 2)\n",
    "# Decision Tree\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "Y_pred5 = decision_tree.fit(X_train, Y_train).predict(X_test)\n",
    "acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n",
    "# Random Forrest\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "Y_pred6 = random_forest.fit(X_train, Y_train).predict(X_test)\n",
    "acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n",
    "#--------------------------------------------------------------------------\n",
    "models = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression','KNN','Support Vector Machines','Naive Bayes','Decision Tree', 'Random Forest'],\n",
    "    'Score': [acc_log, acc_knn, acc_svm,  acc_nb,acc_decision_tree,acc_random_forest]})\n",
    "models=models.sort_values(by='Score', ascending=False)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Score', y = 'Model', data = models)\n",
    "plt.title('Machine Learning Algorithm Accuracy Score \\n')\n",
    "plt.xlabel('Accuracy Score (%)')\n",
    "plt.ylabel('Algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_id,\n",
    "        \"Survived\": Y_pred1\n",
    "    })\n",
    "\n",
    "submission.to_csv('Submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#X_train=X_train.drop([\"Fare\",\"Embarked\",\"Title\",\"IsAlone\"], axis=1)\n",
    "X_train=np.array(X_train).T\n",
    "Y_train=np.array(Y_train)\n",
    "#X_test=X_test.drop([\"Fare\",\"Embarked\",\"Title\",\"IsAlone\"], axis=1)\n",
    "X_test=np.array(X_test).T\n",
    "print(X_train.shape,Y_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitializeParameters(Layers):\n",
    "    np.random.seed(1)\n",
    "    parameters={}\n",
    "    for i in range (len(Layers)-1):\n",
    "        parameters['W'+str(i+1)]=np.random.randn(Layers[i+1],Layers[i])*np.sqrt(2/Layers[i])       #Xavier Initialization\n",
    "        parameters['b'+str(i+1)]=np.zeros((Layers[i+1],1))                                         #Zero Initialization\n",
    "    L=int(len(parameters)/2)\n",
    "    return L,parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RELU(Z):\n",
    "    return np.maximum(0,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    #Z=-np.ones(np.shape(Z))*Z\n",
    "    return 1/(1+np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ForwardProp(X,parameters,L):\n",
    "    cache={}\n",
    "    A=X\n",
    "    cache['A'+str(0)]=A\n",
    "    for i in range (L-1):\n",
    "        Z=np.dot(parameters['W'+str(i+1)],A)+parameters['b'+str(i+1)]\n",
    "        A=RELU(Z)\n",
    "        cache['Z'+str(i+1)]=Z\n",
    "        cache['A'+str(i+1)]=A\n",
    "        cache['W'+str(i+1)]=parameters['W'+str(i+1)]\n",
    "        cache['b'+str(i+1)]=parameters['b'+str(i+1)]\n",
    "    Z=np.dot(parameters['W'+str(L)],A)+parameters['b'+str(L)]\n",
    "    A=sigmoid(Z)\n",
    "    cache['Z'+str(L)]=Z\n",
    "    cache['A'+str(L)]=A\n",
    "    cache['W'+str(L)]=parameters['W'+str(L)]\n",
    "    cache['b'+str(L)]=parameters['b'+str(L)]\n",
    "    return cache,A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(arrayLabel,arrayActivated):\n",
    "    loss=-(1/m)*np.sum(arrayLabel*np.log(arrayActivated)+(1-arrayLabel)*np.log(1-arrayActivated))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGrad(array):\n",
    "    return sigmoid(array)*(1-sigmoid(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RELUGrad(dA,Z):\n",
    "    dZ=np.array(dA, copy=True)\n",
    "    dZ[Z<=0]=0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backProp(X,Y,cache,A):\n",
    "    L=int(len(cache)/4)\n",
    "    bCache={}\n",
    "    dA=-np.divide(Y,A)+np.divide(1-Y,1-A)\n",
    "    dZ=dA*sigmoidGrad(cache['Z'+str(L)])\n",
    "    bCache[\"dW\"+str(L)]=(1/m)*np.dot(dZ,cache['A'+str(L-1)].T)\n",
    "    bCache[\"db\"+str(L)]=(1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA=np.dot(cache['W'+str(L)].T,dZ)\n",
    "    for i in reversed(range(1,L)):\n",
    "        dZ=RELUGrad(dA,cache['Z'+str(i)])\n",
    "        bCache[\"dW\"+str(i)]=(1/m)*np.dot(dZ,cache['A'+str(i-1)].T)\n",
    "        bCache[\"db\"+str(i)]=(1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "        dA=np.dot(cache['W'+str(i)].T,dZ)\n",
    "    return bCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParameters(learningRate,Parameters,Gradients,L):\n",
    "    for i in range(1,L+1):\n",
    "        Parameters['W'+str(i)]=Parameters['W'+str(i)]-learningRate*Gradients[\"dW\"+str(i)]\n",
    "        Parameters['b'+str(i)]=Parameters['b'+str(i)]-learningRate*Gradients[\"db\"+str(i)]\n",
    "    return Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneCycleUpdate(X,Y,parameters,L,learningRate):\n",
    "    cache,A=ForwardProp(X,parameters,L)\n",
    "    cost=costFunction(Y,A)\n",
    "    Gradients=backProp(X,Y,cache,A)\n",
    "    parameters=updateParameters(learningRate,parameters,Gradients,L)\n",
    "    return cost,parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepNeuralNetwork(X,Y,Layers,epochs,learningRate,gradientDescent):\n",
    "    L,parameters=InitializeParameters(Layers)\n",
    "    costs=[]\n",
    "    if gradientDescent==\"BatchGradient\":\n",
    "        for i in range(epochs):\n",
    "            cost,parameters=oneCycleUpdate(X,Y,parameters,L,learningRate)\n",
    "            if i%20==0:\n",
    "                costs.append(cost)\n",
    "            if i%1000==0:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        plotGraph(costs,learningRate)\n",
    "    if gradientDescent==\"MiniBatchGradient\":\n",
    "        \n",
    "        m=X.shape[1]\n",
    "        miniBatchSize=16\n",
    "        miniBatches=[]\n",
    "        numCompleteMinibatches=math.floor(m/miniBatchSize)\n",
    "        \n",
    "        for i in range(numCompleteMinibatches):\n",
    "            mbX=X[:,i*miniBatchSize:(i+1)*miniBatchSize]\n",
    "            mbY=Y[i*miniBatchSize:(i+1)*miniBatchSize]\n",
    "            miniBatches.append([mbX,mbY])\n",
    "\n",
    "        if m%miniBatchSize!=0:\n",
    "            mbX=X[:,numCompleteMinibatches*miniBatchSize:m]\n",
    "            mbY=Y[numCompleteMinibatches*miniBatchSize:m]\n",
    "            miniBatches.append([mbX,mbY]) \n",
    "        for i in range(epochs):\n",
    "            cost=0\n",
    "            for minibatch in miniBatches:\n",
    "                [mX,mY]=minibatch\n",
    "                cost,parameters=oneCycleUpdate(mX,mY,parameters,L,learningRate)\n",
    "                cost/=m\n",
    "            if i%10==0:\n",
    "                costs.append(cost)\n",
    "            if i%100==0:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        plotGraph(costs,learningRate)\n",
    "    return L,parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotGraph(costs,learningRate):\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.title(\"Learning rate =\" + str(learningRate))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,m=X_train.shape\n",
    "#C=3\n",
    "Layers=[7,5,6,4,7,8,2,1]\n",
    "#When BatchGradient\n",
    "#epoch,learningRate=100000,0.002\n",
    "#When MiniBatchGradient\n",
    "epoch,learningRate=1000,0.5\n",
    "L,parameters=deepNeuralNetwork(X_train,Y_train,Layers,epoch,learningRate,gradientDescent=\"MiniBatchGradient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,Y,parameters,L):\n",
    "    _,A=ForwardProp(X,parameters,L)\n",
    "    A=(A+0.5).astype(int)\n",
    "    #print(A.shape,A)\n",
    "    s=0\n",
    "    Y=Y.reshape(1,891)\n",
    "    #print(Y.shape,Y)\n",
    "    for i in range(len(Y_train)):\n",
    "        if Y[0,i]==A[0,i]:\n",
    "            s+=1\n",
    "    return (s/m)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=predict(X_train,Y_train,parameters,L)\n",
    "print(\"Prediction percentage on training set is \"+str(prediction)+\" %.\\nError in training set is \"+str(100-prediction)+\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,A=ForwardProp(X_test,parameters,L)\n",
    "A=(A+0.5).astype(int).reshape(418).tolist()\n",
    "print(np.array(A).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_test['PassengerId'])\n",
    "submission = pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':A})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Titanic Predictions 1.csv'\n",
    "submission.to_csv(filename,index=False)\n",
    "print('Saved file: ' + filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
